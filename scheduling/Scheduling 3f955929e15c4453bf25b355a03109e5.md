# Scheduling

### Manual Scheduling

- Use the `nodeName` field of `spec` in the Pod definition to manually schedule a Pod to a Node
    - The scheduler automatically sets this field for Pods that do not have it set and have to undergo scheduling
    - If you want to bind a created Pod to a Node then you have to create a Binding object and send it to the binding API

Links: 

- [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

### Labels and Selectors

- Labels help us tag Objects and Selectors help in filtering them
- Labels are set under the metadata part of the definition file under `labels` with a key-value format
- Selectors are used for example in a ReplicaSet to know which pods to take under it
    - Selectors are specified in the `spec` part of the definition file under `selector` with either `matchLabels` or/and `matchExpressions`

Links:

- [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
- [https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)

### Taints and Tolerations

- Taints and Tolerations specify which Pods can be scheduled on a Node
- If we put a Taint on a Node, then only Pods with Tolerations to that Taint can be scheduled to that specific Node
    - This does NOT guarantee that the Pod in question will be scheduled on that Node
- You can Taint a Node with the command `kubectl taint nodes $NODE_NAME $KEY=$VALUE:$TAINT_EFFECT`
    - The Taint effect specifies what happens to Pods that do not tolerate the Taint
        - NoSchedule - the Pods will not be scheduled on the Node
        - PreferNoSchedule - the system will try to not place the Pod on the Node
        - NoExecute - new Pods will not be scheduled on the Node and existing pods which do not tolerate the Taint will be evicted
- Tolerations can be set on the Pod under the `spec` section
    
    ```yaml
    tolerations:
      - key: app
        operator: Equal
        value: blue
        effect: NoSchedule
    ```
    

Links:

- [https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)

### NodeSelector

- In the Pod definition file under the `spec` section you can specify key-value pairs which the scheduler will take into account when scheduling the Pod
    - The key-value pair has to exist on a Node as a Label to make sure the Pod gets scheduled on that specific Node

Links:

- [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)

### NodeAffinity

- NodeAffinity is a more advanced NodeSelector which allows us to define more specific rules of mapping a Pod to a Node
- NodeAffinity can be set in the Pod definition file under the `spec` section
    
    ```yaml
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
              - key: size
                operator: In
                values:
                - Small
    ```
    
- The NodeAffinity type specifies the behavior of the scheduler:
    - requiredDuringSchedulingIgnoredDuringExecution - the expressions must match for a Pod to be placed on the Node but if the labels change on the Node during runtime then the Pods won’t be evicted from the Node
    - preferredDuringSchedulingIgnoredDuringExecution - the scheduler will try to find a Node that matches the expressions but will schedule it on a different Node if the expressions are not met

Links:

- [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)

### Node Affinity vs. Taints and Tolerations

- Taints and Tolerations let us specify that only Pods with a specific Toleration to the Taint on the Node get scheduled on the Nodes, but this does not guarantee that the Pods don’t get scheduled on other Nodes
- NodeAffinity lets us specify that certain Pods only get scheduled to the Node with the specific labels but does not guarantee that other Pods don’t get scheduled on the Node
- The combination of the two lets us make sure that ONLY the specific Pods get scheduled on that Node and none other

### Resource Requirements and Limits

- The scheduler takes into account the resource requirements of a Pod and if the Node does not have sufficient resources then the Pod won’t be scheduled on it
- The default resource requirements are 0.5 CPU and 256 Mi Memory
- Resource **requirements** can be specified in the Pod definition file under `containers` in the `spec` section
    
    ```yaml
    resource:
      requests:
        memory: "1Gi"
        cpu: 1
    ```
    
- G is Gigabyte, while Gi stands for Gibibyte, same for M, K and Mi, Ki
- By default the CPU limit of a Pod is 1 vCPU, the default memory limit is 512 Mi
- Resource **limits** can be specified in the Pod definition file under `containers` in the `spec` section
    
    ```yaml
    resource:
      requests:
    		memory: "1Gi"
    		cpu: 1
    	limits:
    		memory: "2Gi"
    		cpu: 2
    ```
    
- If a Pod tries to exceed the CPU limits k8s throttles the Pod if it tries to exceed the memory limits then the Pod is terminated
- The default values for limits and requests can be configured with a LimitRange object in the Namespace

Links:

- [https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
- [https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
- [https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/](https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/)
- [https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/](https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/)

### DaemonSets

- DaemonSets runs a copy of a Pod on each Node, whenever a new Node is added to the Cluster then a Pod will also be scheduled on that Node
- The **kube-proxy** and networking solutions are also deployed to every Node using this approach
- DaemonSets have a very similar definition structure to ReplicaSets
- DaemonSets use NodeAffinity and the default scheduler to schedule a Pod on every Node

Links:

- [https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)

### Static Pods

- The kubelet can manage a Node independently in case there are no other Worker and/or Master Nodes
- The kubelet can be configured to read Pod definition files from a folder on the server designed for this scenario
- The kubelet periodically checks this folder and creates the Pods and applies changes to them in case the files are changed, note that only Pods can be created this way
- The folder is configured as an option passed to the kubelet on start as `--pod-manifest-path=$PATH`
    - It can also be configured with specifying the `--config=$CONFIG_FILE` option on the kubelet and in the config file specifying the `staticPodPath: $PATH`, Clusters set up with kubeadm use this approach
- Static Pods can be viewed with the `docker ps` command when there is no API Server in the Cluster
- If there is an API Server in the Cluster then you can identify Static Pods by their names, the Nodes’ names are appended to the Pods’ names (note that these Pods are read-only copies of the Static Pods, they cannot be modified using kubectl)
- Both Static Pods and DaemonSets are ignored by the kube-scheduler and while Static Pods are created by the kubelet, DaemonSets are created by the API Server (DaemonSet Controller)
- The use case of Static Pods is that you can easily deploy Control Plane components as Static Pods while with DaemonSets you can deploy Logging and Monitoring Agents on every Node for example

Links:

- [https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/)